# Повторение результатов статьи 

Повторяем эксперимент из статьи

```
Bunel, R., Hausknecht, M., Devlin, J., Singh, R., & Kohli, P. (2018). 
Leveraging grammar and reinforcement learning for neural program synthesis. 
arXiv preprint arXiv:1805.04276.
```

В статье предлагается использовать обучение с подкреплением и отсев генерируемых вариантов программ путем из их исполнения и проверки синтаксиса.

Используется сгенерированный датасет [karel_dataset](https://msr-redmond.github.io/karel-dataset/).

Исходный код для начала эксперимента доступен на [Github](https://github.com/bunelr/GandRL_for_NPS).

## Шпаргалка

См. подсказки по [сложным действиям](./FAQ.md)

## Как сдавать

 - [ ] решить задания по задачам, ответы на вопросы привести в этом файле или добавить в нужном пункте ссылку на ответ
 - [ ] оформить в виде git репозитория, в котором размещены все результаты
 - [ ] прислать ссылку на репозиторий письмом
 
Пример оформления ответа

 - как оформить ответ на вопрос?
> Например вот так
 - как добавить ответ ссылкой
> вот [ссылка](./TASK.md#как-сдавать)


Для обучения моделей можно использовать Google Colab c GPU.

## Задача 1. Подготовка данных

### Задание 1. 

Получите karel_dataset, найдите в нем train.json и разберитесь в его структуре.

Вопросы
 - как правильно называется формат файла train.json?
 >Такой формат датасета называется JSON Lines или JSONL.
 - как взять часть из файла train.json?
 >Мне ресурсы позволяют, поэтому я считал из него все и сделал `random.sample` нужных размеров
 - подготовьте файлы корректного формата размером 1%, 3%, 10% от оригинала train.json
 >[Ссылка на диск](https://disk.yandex.ru/d/OGxoBTuQchk0jQ)
 
### Задание 2. 
 
Напишите код, который загружает данные из train.json
 
Вопросы
 - оцените объем необходимой RAM
 > Ну объем нужной оперативки сравним с размером файла `~10Gb`
 - реализуйте загрузку в итератор словарей (паттерн итератор)
  
## Задача 2. Подготовка репозитория 

### Задание 1. 

Получите GandRL_for_NPS из github и смерджите его в этот репозиторий (см. [как это сделать](./FAQ.md#как-объединить-репозитории) в FAQ.md)

Вопросы
 - на каком языке реализован?
 > Python2
 - что нужно сделать для установки эксперимента и зависимостей?
 - где должны быть размещены данные?
 > Куда положите, там и будут лежать. Пути до тренировочного датасета, валидационного и словарей указываются в ручках `--train_file, --val_file, --vocab_file` 
 - что нужно сделать для запуска эксперимента, как указать параметры и какие значения выбрать?
 > Параметры указываются в аргументах при запуске `train_cmd.py`. Потом они дампятся в логи. Из того, что указано в примере в репозитории: 
 > * `--kernel_size`, `--conv_stack`, `--fc_stack`, `--tgt_embedding_size`,
   `--lstm_hidden_size`, `--nb_lstm_layers` — это флаги для указания архитектуры.
   модели для изучения. 
> * `--nb_ios` указывает, сколько пар ввода-вывода должно использоваться в качестве входов для
   кодера.
> * `--use_grammar` заставляет модель использовать рукописную проверку синтаксиса, которую можно найти в
   `синтаксис/checker.pyx`. 
> * `--learn_syntax` добавляет Syntax LSTM в модель, которая
   пытается изучить средство проверки синтаксиса совместно с остальной частью модели. 
> * `--signal` позволяет выбирать loss между `supervised`, `rl` и
   `beam_rl`. 
> * `--environment` - какие награды используются
   указывается с помощью аргумента  (это может быть Consistency для
   оценки согласованность программ с наблюдаемыми сетками ввода-вывода, или Perf -
   учитывается количество предпринятых шагов.) 
> * `--optim_alg` выбирает используемый алгоритм оптимизации.
> * `--batch_size` позволяет
   выбрать размер батча. 
>* `--learning_rate` регулирует learning_rate
>* `--init_weights` может использоваться для указания файла '.model', из которого будут загружены веса.
> * `--train_file` указать jsonl-файл для обучения
>* `--val_file` указывает jsonl-файл для валидации. 
>* `--vocab` словарь токенов модели
>* `--nb_samples` позволяет тренироваться только на части
   набора данных (0, по умолчанию, обучается по всему набору данных).
>* `--result_folder` позволяет указать, где находятся результаты эксперимента. 
> * `--use_cuda` чтобы все запускалось на графическом процессоре. Можно указать конкретный gpu через 
   `CUDA_VISIBLE_DEVICES` 
 - что нужно сделать для проверки обученной модели?
> * Ну можно попробовать сгенерировать какую-нибудь программу и посмотреть, что будет. А вообще модель каждую эпоху валидируется...
### Задание 2. 
 
Загрузите 1% от train.json и остальные файлы из karel_dataset
 
Вопросы
 - зачем нужны файлы *.thdump в папке датасета?
 > Туда torch-like датасет кладется, и если он существует, то он его поднимает и на нем обучается
 - что содержит new_vocab?
 > `--vocab` словарь токенов модели
 - где находится датасет для контроля и для теста?
 > Валидационный передается в `--val_file`
 - как устроен экземпляр данных для обучения?
 > [Train sample](./sample.json)

### Задание 3. 

Проведите эксперимент с 1% данных

Вопросы
 - как указать вид модели?
 - какие ошибки возникли при запуске и как вы их устранили?
 > Репозиторий старый, корректно работает только на `pytorch:0.3.1`, при этом CUDA для этого торча максимальная - CUDA9. У меня есть только RTX4090, что очевидно слишком молодо для нас. Хорошо, можно пойти в colab, но нет, ибо в колабе стоят Tesla T4, которые не поддежривают корректно CUDA9. В итоге я психанул и запустился на CPU
 ---
 >Еще в `nps/data.py` на моменте 

 ``` 
 for inp_grid_desc, out_grid_desc in sample[:nb_ios]: 
 ``` 
 > он падает, ибо пытается больше параметров распаковать, причем только на моменте валидации. Поменял на 
 ```
 for i in sample[:nb_ios]:
            inp_grid_desc, out_grid_desc = i[:2]
```
 - сколько эпох вы провели?
 > Мало, всего 4. Но с учетом производительности торча на CPU, провести больше можно, но это будет очень долго, а дождаться какого-нибудь адекватного accuracy практически невозможно
 - где сохранены результаты и логи эксперимента?
 > [Папка с результатами](./exps/) 
 - какого качества получен результат?
 > Качество нулевое, но loss на обучении падает. Мало эпох, подозреваю, но больше запускать не на чем
 
3а. Оператор m += 1 имеет другое значение, чем m = m + 1. Также воспользуйтесь явным приведением типов
 
3б. Из-за разных версий torch может потребоваться адаптировать код проекта. Например, для получения значения из тензора размером 1 нужно вызывать ``item()`` вместо ``[0]``

3в. Удалите кэши данных перед запуском

3г. Попробуйте заменить tqdm на progressbar2, см. [вот это сообщение](https://github.com/tqdm/tqdm/issues/613) или установить версию 2018 года.

3д. После обновления кода не забывайте установить повторно через setup.py

 
### Задание 4. 
 
Сделайте свой клон репозитория с исправлениями, добавьте архивы подготовленных данных 1%, 3%, 10% на файлообменник с доступом по прямой ссылке
 
Вопросы
 - как получить данные по ссылке из командной строки?
 > Приблизительно также, как и датасет из задания. Перехватить запросы браузера и в одном из ответов Яндекс.Диск вернет прямую ссылку
 - где находится ваш репозиторий?
 > [Ссылка на репозиторий](https://github.com/chupolino17/autoprog_hw4)

### Задание 5*.

Подключите журналирование кривых обучения и промежуточных результатов в [TensorboardX](https://github.com/lanpa/tensorboardX). 
Для этого нужно использовать SummaryWriter для сохранения промежуточных значений функции потерь внутри цикла обучения. Например, после каждого минибатча или 100 миниматчей. 

Логи сохранять в подпапку ``runs`` папки эксперимента в ``./exps/*``

Для использования установить Tensorboard и указать данную папку в качестве исходной.
  
## Задача 3. Анализ и повторение результата

### Задание 1. 

Проведите эксперименты для 1%, 3%, 10% данных для MLE (supervised), RL_beam и обучаемой и предзаданной моделью синтаксиса

Вопросы
 - сколько времени заняло проведение эксперимента, какие ресурсы использовали?
 > `float('inf')` ибо это почти наверное невозможно в 2023 году. На четыре эпохи на 1% для MLE это заняло 1,5 часа. Можно попробовать посчитать сколько займет проведение всех экспериментов: `(MLE | RL_beam) * (use_grammar | learn_syntax) * (1% | 3%(x3) | 10% (x10)) * 0,3 часа\эпоху * (~100 эпох из примера в репозитории) = 1680 часов (70 дней)`
 - какого качества удалось достичь?
 > Accuracy - 0ю
 - приведите 10 примеров синтеза программы для karel обученной моделью

Начните с MLE и используйте обученную таким образом модель для RL в качестве начального приближения.
См. подробнее в репозитории проекта.



